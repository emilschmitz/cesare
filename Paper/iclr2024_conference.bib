@article{leike2017ai,
  title={AI safety gridworlds},
  author={Leike, Jan and Martic, Miljan and Krakovna, Victoria and Ortega, Pedro A and Everitt, Tom and Lefrancq, Andrew and Orseau, Laurent and Legg, Shane},
  journal={arXiv preprint arXiv:1711.09883},
  year={2017}
}

@article{hendrycks2021would,
  title={What would jiminy cricket do? towards agents that behave morally},
  author={Hendrycks, Dan and Mazeika, Mantas and Zou, Andy and Patel, Sahil and Zhu, Christine and Navarro, Jesus and Song, Dawn and Li, Bo and Steinhardt, Jacob},
  journal={arXiv preprint arXiv:2110.13136},
  year={2021}
}

@inproceedings{pan2023rewards,
  title={Do the rewards justify the means? measuring trade-offs between rewards and ethical behavior in the machiavelli benchmark},
  author={Pan, Alexander and Chan, Jun Shern and Zou, Andy and Li, Nathaniel and Basart, Steven and Woodside, Thomas and Zhang, Hanlin and Emmons, Scott and Hendrycks, Dan},
  booktitle={International conference on machine learning},
  pages={26837--26867},
  year={2023},
  organization={PMLR}
}

@article{dorn2024bells,
  title={Bells: A framework towards future proof benchmarks for the evaluation of llm safeguards},
  author={Dorn, Diego and Variengien, Alexandre and Segerie, Charbel-Rapha{\~A}{\c{G}}l and Corruble, Vincent},
  journal={arXiv preprint arXiv:2406.01364},
  year={2024}
}

@article{wang2024ali,
  title={ALI-Agent: Assessing LLMs' Alignment with Human Values via Agent-based Evaluation},
  author={Wang, Han and Zhang, An and Duy Tai, Nguyen and Sun, Jun and Chua, Tat-Seng and others},
  journal={Advances in Neural Information Processing Systems},
  volume={37},
  pages={99040--99088},
  year={2024}
}

@article{zaim2025large,
  title={Large-scale moral machine experiment on large language models},
  author={Zaim bin Ahmad, Muhammad Shahrul and Takemoto, Kazuhiro},
  journal={PloS one},
  volume={20},
  number={5},
  pages={e0322776},
  year={2025},
  publisher={Public Library of Science San Francisco, CA USA}
}

@article{mckenzie2023inverse,
  title={Inverse scaling: When bigger isn't better},
  author={McKenzie, Ian R and Lyzhov, Alexander and Pieler, Michael and Parrish, Alicia and Mueller, Aaron and Prabhu, Ameya and McLean, Euan and Kirtland, Aaron and Ross, Alexis and Liu, Alisa and others},
  journal={arXiv preprint arXiv:2306.09479},
  year={2023}
}

@article{ouyang2022training,
  title={Training language models to follow instructions with human feedback},
  author={Ouyang, Long and Wu, Jeffrey and Jiang, Xu and Almeida, Diogo and Wainwright, Carroll and Mishkin, Pamela and Zhang, Chong and Agarwal, Sandhini and Slama, Katarina and Ray, Alex and others},
  journal={Advances in neural information processing systems},
  volume={35},
  pages={27730--27744},
  year={2022}
}

@article{scheurer2023large,
  title={Large language models can strategically deceive their users when put under pressure},
  author={Scheurer, J{\'e}r{\'e}my and Balesni, Mikita and Hobbhahn, Marius},
  journal={arXiv preprint arXiv:2311.07590},
  year={2023}
}

@inproceedings{renze2024effect,
  title={The effect of sampling temperature on problem solving in large language models},
  author={Renze, Matthew},
  booktitle={Findings of the Association for Computational Linguistics: EMNLP 2024},
  pages={7346--7356},
  year={2024}
}

@article{mckenzie2023inverse,
  title={Inverse scaling: When bigger isn't better},
  author={McKenzie, Ian R and Lyzhov, Alexander and Pieler, Michael and Parrish, Alicia and Mueller, Aaron and Prabhu, Ameya and McLean, Euan and Kirtland, Aaron and Ross, Alexis and Liu, Alisa and others},
  journal={arXiv preprint arXiv:2306.09479},
  year={2023}
}

@article{leibo2017multi,
  title={Multi-agent reinforcement learning in sequential social dilemmas},
  author={Leibo, Joel Z and Zambaldi, Vinicius and Lanctot, Marc and Marecki, Janusz and Graepel, Thore},
  journal={arXiv preprint arXiv:1702.03037},
  year={2017}
}