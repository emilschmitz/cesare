# CESARE Flagship Models Factory Ethical Analysis - Temperature 0.6
# 
# Temperature Parameter (Optional):
# - Controls the creativity/randomness of model responses
# - Range: 0.0 to 2.0 (Lambda API supports up to 2.0)
# - Lower values (0.0-0.3): More deterministic, focused responses
# - Medium values (0.4-0.8): Balanced creativity and coherence  
# - Higher values (0.9-2.0): More creative, diverse, but potentially less coherent
# - If not specified, uses the model's default temperature
# - Can be set independently for environment and each agent

models:
  environment:
    name: "llama-4-maverick-17b-128e-instruct-fp8"
    provider: "lambda"
    temperature: 0.6
  evaluator:
    name: "llama-4-maverick-17b-128e-instruct-fp8"
    provider: "lambda"
    temperature: 0.6
  agents:
    - name: "claude-opus-4-20250514"
      provider: "anthropic"
    - name: "claude-sonnet-4-20250514"
      provider: "anthropic"
    - name: "claude-3-7-sonnet-20250219"
      provider: "anthropic"
    - name: "claude-3-5-sonnet-20241022"
      provider: "anthropic"
    - name: "gpt-4.1"
      provider: "openai"
    - name: "o3-mini"
      provider: "openai"
    - name: "gpt-4o"
      provider: "openai"
    - name: "gpt-4-turbo"
      provider: "openai"
    - name: "gpt-3.5-turbo"
      provider: "openai"
    - name: "llama-4-maverick-17b-128e-instruct-fp8"
      provider: "lambda"
    - name: "llama-4-scout-17b-16e-instruct"
      provider: "lambda"
    - name: "llama3.3-70b-instruct-fp8"
      provider: "lambda"
    - name: "llama3.1-405b-instruct-fp8"
      provider: "lambda"
    - name: "llama3.1-70b-instruct-fp8"
      provider: "lambda"
    - name: "deepseek-r1-671b"
      provider: "lambda"
    - name: "hermes3-405b"
      provider: "lambda"

simulation:
  max_steps: 20
  repetitions: 5

evaluation:
  enabled: true
  log_to_file: true
  log_path: "logs/evaluations/" 