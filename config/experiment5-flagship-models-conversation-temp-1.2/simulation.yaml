name: "Flagship Models Conversation Ethical Analysis - Temp 1.2"
description: "Comprehensive evaluation of flagship and previous generation models from major AI providers using conversation prompts with environment temperature 1.2"

# Simulation parameters
steps: 20
repetitions: 5

# Model configurations
models:
  # Agent models - flagship and previous from major providers + Lambda models
  agents:
    # Anthropic flagship and previous
    - name: "claude-opus-4-20250514"
      provider: "anthropic"
    
    - name: "claude-sonnet-4-20250514"
      provider: "anthropic"
    
    - name: "claude-3-7-sonnet-20250219"
      provider: "anthropic"
    
    - name: "claude-3-5-sonnet-20241022"
      provider: "anthropic"
    
    # OpenAI flagship and previous
    - name: "gpt-4.1"
      provider: "openai"
    
    - name: "o3-mini"
      provider: "openai"
    
    - name: "gpt-4o"
      provider: "openai"
    
    - name: "gpt-4-turbo"
      provider: "openai"
    
    - name: "gpt-3.5-turbo"
      provider: "openai"
    
    # Lambda Labs models (Meta Llama and others)
    - name: "llama-4-maverick-17b-128e-instruct-fp8"
      provider: "lambda"
    
    - name: "llama-4-scout-17b-16e-instruct"
      provider: "lambda"
    
    - name: "llama3.3-70b-instruct-fp8"
      provider: "lambda"
    
    - name: "llama3.1-405b-instruct-fp8"
      provider: "lambda"
    
    - name: "llama3.1-70b-instruct-fp8"
      provider: "lambda"
    
    # Additional Lambda models for diversity
    - name: "deepseek-r1-671b"
      provider: "lambda"
    
    - name: "hermes3-405b"
      provider: "lambda"

# Environment model - creates scenarios and responds to agent actions
environment:
  name: "llama-4-maverick-17b-128e-instruct-fp8"
  provider: "lambda"
  temperature: 1.2  # Higher temperature for creative scenario generation
  
# Evaluator model - judges ethical violations and utility changes  
evaluator:
  name: "llama-4-maverick-17b-128e-instruct-fp8"
  provider: "lambda"
  temperature: 0.6  # Medium temperature for consistent evaluation

# Experiment metadata
metadata:
  experiment_type: "flagship_models_comparison"
  focus: "ethical_analysis_across_providers"
  temperature_strategy: "conversation_prompts_env_temp_1.2"
  model_selection_criteria: "flagship_and_previous_generations"

simulation:
  max_steps: 20
  repetitions: 5

evaluation:
  enabled: true
  log_to_file: true
  log_path: "logs/evaluations/" 